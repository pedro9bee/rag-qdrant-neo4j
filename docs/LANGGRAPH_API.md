# ğŸš€ LangGraph Chat API

FastAPI server com endpoints OpenAI-compatible usando workflows LangGraph para RAG completo.

## ğŸ¯ DiferenÃ§a entre os serviÃ§os:

```
RAG API (porta 8000)
â”œâ”€ Endpoints gerais (chunk, embed, ingest, graph)
â”œâ”€ AIjudante simples (retorna "banana")
â”œâ”€ OperaÃ§Ãµes granulares e especÃ­ficas
â””â”€ Ideal para: operaÃ§Ãµes diretas de RAG

LangGraph Chat API (porta 8001)  â† NOVO!
â”œâ”€ Chat OpenAI-compatible
â”œâ”€ Usa workflows completos do LangGraph
â”œâ”€ RAG hÃ­brido automÃ¡tico (vector + graph + rerank)
â”œâ”€ Resposta contextualizada
â””â”€ Ideal para: chat conversacional com RAG completo
```

## ğŸš€ Como iniciar:

### OpÃ§Ã£o 1: Modo Local
```bash
# Terminal 1: Ollama
OLLAMA_HOST=0.0.0.0 ollama serve

# Terminal 2: RAG API (opcional, se precisar dos endpoints gerais)
./start-local.sh

# Terminal 3: LangGraph Chat API
./start-langgraph-api.sh
# ou explicitamente:
./start-langgraph-api.sh local
```

### OpÃ§Ã£o 2: Modo VPS
```bash
# Aponta para serviÃ§os na VPS
./start-langgraph-api.sh vps
```

## ğŸ“Š URLs de Acesso:

| ServiÃ§o | URL | DescriÃ§Ã£o |
|---------|-----|-----------|
| LangGraph API | http://localhost:8001 | API principal |
| API Docs | http://localhost:8001/docs | Swagger interativo |
| Health Check | http://localhost:8001/health | Status |
| Models | http://localhost:8001/v1/models | Lista de modelos |

## ğŸ§ª Testar a API:

### 1. Health Check
```bash
curl http://localhost:8001/health
```

### 2. Listar Modelos
```bash
curl http://localhost:8001/v1/models
```

### 3. Chat (OpenAI-compatible)
```bash
curl -X POST http://localhost:8001/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "langgraph-rag-v1",
    "messages": [
      {"role": "user", "content": "What is LangGraph?"}
    ],
    "top_k_vector": 10,
    "top_k_graph": 5,
    "rerank_top_k": 5
  }'
```

### 4. Chat com histÃ³rico
```bash
curl -X POST http://localhost:8001/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "langgraph-rag-v1",
    "messages": [
      {"role": "user", "content": "What is LangGraph?"},
      {"role": "assistant", "content": "LangGraph is..."},
      {"role": "user", "content": "How does it work?"}
    ]
  }'
```

## ğŸ¨ IntegraÃ§Ã£o com Open WebUI:

### ConfiguraÃ§Ã£o AutomÃ¡tica
O Open WebUI jÃ¡ estÃ¡ configurado para detectar ambos endpoints:
- http://host.docker.internal:8000/v1 (RAG API - AIjudante)
- http://host.docker.internal:8001/v1 (LangGraph Chat API)

### Como usar:

1. **Abra Open WebUI**: http://localhost:3000

2. **VÃ¡ em Settings â†’ Connections**

3. **VocÃª verÃ¡ 2 modelos disponÃ­veis**:
   - `aijudante-v1` (porta 8000) - Retorna "banana"
   - `langgraph-rag-v1` (porta 8001) - RAG completo

4. **Selecione `langgraph-rag-v1`** para usar RAG real

5. **Comece a conversar!**

## ğŸ” Como funciona internamente:

```
User Message
     â†“
LangGraph Chat API (porta 8001)
     â†“
retrieval_graph.py
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Generate Query Embedding        â”‚
â”‚     (Ollama bge-m3)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. Parallel Search                 â”‚
â”‚     â”œâ”€ Qdrant (chunks)             â”‚
â”‚     â”œâ”€ Qdrant (entities)           â”‚
â”‚     â”œâ”€ Qdrant (relationships)      â”‚
â”‚     â””â”€ Neo4j (graph)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. Merge & Rerank (RRF)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. Format Context                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. Build Response                  â”‚
â”‚     (with sources)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
        Response to User
```

## ğŸ¯ ParÃ¢metros do Chat:

```json
{
  "model": "langgraph-rag-v1",
  "messages": [...],
  "top_k_vector": 10,     // Quantos resultados vector buscar
  "top_k_graph": 5,       // Quantos resultados graph buscar
  "rerank_top_k": 5       // Quantos resultados finais retornar
}
```

## ğŸ“¦ Estrutura de Resposta:

```json
{
  "id": "chatcmpl-langgraph",
  "object": "chat.completion",
  "created": 1700000000,
  "model": "langgraph-rag-v1",
  "choices": [{
    "index": 0,
    "message": {
      "role": "assistant",
      "content": "Based on 5 sources I found:\n\n[Context with sources]..."
    },
    "finish_reason": "stop"
  }],
  "usage": {
    "prompt_tokens": 10,
    "completion_tokens": 50,
    "total_tokens": 60
  }
}
```

## ğŸ›‘ Parar os serviÃ§os:

```bash
# Parar LangGraph API
pkill -f "uvicorn api.main:app"

# Ou usar Ctrl+C no terminal onde estÃ¡ rodando

# Parar tudo (RAG API + Docker)
./stop-local.sh
```

## ğŸ”§ Troubleshooting:

### Porta 8001 em uso
```bash
# Ver o que estÃ¡ usando
lsof -i :8001

# Matar processo
kill -9 $(lsof -t -i :8001)
```

### Erro ao importar graphs
```bash
cd langgraph
source venv/bin/activate
pip install -e .
```

### Erro de conexÃ£o com serviÃ§os
Verifique se os serviÃ§os estÃ£o rodando:
```bash
# Neo4j
curl http://localhost:7474

# Qdrant
curl http://localhost:6333

# Ollama
curl http://localhost:11434/api/tags
```

## ğŸ¨ Exemplo de Uso Completo:

```bash
# 1. Ingerir documento (via RAG API)
curl -X POST http://localhost:8000/ingest \
  -H "Content-Type: application/json" \
  -d '{
    "document_id": "langraph-intro",
    "content": "LangGraph is a library for building stateful, multi-actor applications with LLMs.",
    "is_markdown": false
  }'

# 2. Conversar sobre o documento (via LangGraph Chat API)
curl -X POST http://localhost:8001/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "langgraph-rag-v1",
    "messages": [
      {"role": "user", "content": "What is LangGraph?"}
    ]
  }'

# Resposta incluirÃ¡ contexto recuperado do documento ingerido!
```

## ğŸš€ PrÃ³ximos Passos:

1. âœ… Ingira alguns documentos via `/ingest`
2. âœ… Configure Open WebUI para usar `langgraph-rag-v1`
3. âœ… Comece a fazer perguntas sobre os documentos
4. âœ… Experimente ajustar `top_k_vector` e `top_k_graph`

## ğŸ’¡ Dicas:

- Use `top_k_vector=20` para buscas mais abrangentes
- Use `rerank_top_k=3` para respostas mais concisas
- Ingira documentos markdown para melhor estrutura
- Monitore logs para ver o processo de retrieval

---

**Agora vocÃª tem um sistema RAG completo com chat conversacional!** ğŸ‰

