# ============================================================================
# RAG Stack Configuration - Local Docker Environment
# ============================================================================
# This file configures connections to services running in local Docker
# Copy this to .env and use it for local development

# ----------------------------------------------------------------------------
# LLM Configuration (Salad Cloud or Local Ollama)
# ----------------------------------------------------------------------------
# OPﾃﾃグ 1 - SALAD CLOUD (RECOMENDADO PARA PRODUﾃﾃグ):
# Cole a URL do seu Container Group do Salad (sem o /v1 no final)
# Exemplo: OLLAMA_BASE_URL=https://eggplant-bean-p6gi8i84t72jewf3.salad.cloud
# 
# OPﾃﾃグ 2 - OLLAMA LOCAL:
# OLLAMA_BASE_URL=http://localhost:11434
# IMPORTANTE: Execute com: OLLAMA_HOST=0.0.0.0 ollama serve

# 圷 ATENﾃﾃグ: Substitua pela URL do seu Salad Cloud Container Group
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_EMBEDDING_MODEL=bge-m3:latest
OLLAMA_ENTITY_MODEL=pedro9bee/uniner-7b-all:gguf-q4
OLLAMA_LLM_MODEL=bartowski/phi-3.5-mini-instruct-q5_k_m:latest
EMBEDDING_DIMENSIONS=1024

# ----------------------------------------------------------------------------
# Entity & Relationship Lists (Knowledge Graph)
# ----------------------------------------------------------------------------
ENTITIES_LIST=Agent,Graph,Node,State,Transition,LLM,Tool,MCP,Gateway,BusinessSystem,BedrockAgentCore,Memory,User,AWS,Bedrock,Lambda,S3,Claude,Haiku,Sonnet,Opus,LangChain,LangGraph,Qdrant,Neo4j,MinIO,Ollama,Kestra,Python,RAG,Embedding,Vector,Chunk,Document

RELATIONSHIPS_LIST=implemented_by,contains,operates_on,updates,uses_for_reasoning,connects,invokes,uses_protocol,routes_to,accesses,executes_in,persists_data_in,interacts_with,offers,uses,provides,integrates_with,depends_on,supports,requires,enables,manages,processes,stores,retrieves,orchestrates,embedded_by,indexes_in,queries

# ----------------------------------------------------------------------------
# RAG Configuration
# ----------------------------------------------------------------------------
CHUNK_SIZE=1000
CHUNK_OVERLAP=200

# ----------------------------------------------------------------------------
# Service URLs (Local Docker - access via localhost mapped ports)
# ----------------------------------------------------------------------------
# MinIO - Local Docker
MINIO_ENDPOINT=http://localhost:9002
MINIO_ACCESS_KEY=minioadmin
MINIO_SECRET_KEY=minioadmin
MINIO_SECURE=false
MINIO_BUCKET=documents

# Qdrant - Local Docker
QDRANT_URL=http://localhost:6333
QDRANT_COLLECTION_NAME=rag_embeddings
QDRANT_API_KEY=

# Neo4j - Local Docker
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=neo4j_password

# Redis - Local Docker
REDIS_URL=redis://localhost:6379
REDIS_JOB_TTL=86400

# ----------------------------------------------------------------------------
# Pipeline Configuration
# ----------------------------------------------------------------------------
# Markdown Chunking
MARKDOWN_CHUNK_SIZE=1000
MARKDOWN_CHUNK_OVERLAP=200
MAX_FILE_SIZE_MB=50

# spaCy NER
SPACY_MODEL=en_core_web_lg
SPACY_CONFIDENCE_THRESHOLD=0.90

# LLM Models for Pipeline
# 識 CONFIGURAﾃﾃグ ATUAL: Mistral via Ollama
ENTITY_EXTRACTION_MODEL=mistral:latest
ENTITY_VALIDATION_MODEL=pedro9bee/uniner-7b-all:gguf-q4
RELATIONSHIP_EXTRACTION_MODEL=mistral-7b-v0.3:latest

# 笨 TESTADO E FUNCIONANDO:
# ENTITY_EXTRACTION_MODEL=llama3.2:3b
# ENTITY_EXTRACTION_MODEL=mistral-7b-v0.3:latest
# ENTITY_EXTRACTION_MODEL=hf.co/bartowski/Qwen2.5-7B-Instruct-GGUF:Q5_K_M
# ENTITY_EXTRACTION_MODEL=hf.co/bartowski/Phi-3.5-mini-instruct-GGUF:Q5_K_M
# ENTITY_EXTRACTION_MODEL=hf.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF:Q5_K_M

# Entity Validation Batch Size (parallel processing)
ENTITY_VALIDATION_BATCH_SIZE=10

# ----------------------------------------------------------------------------
# API Configuration
# ----------------------------------------------------------------------------
DEBUG=false

