id: retrieval_rag
namespace: ai.rag

description: |
  RAG Hybrid Retrieval Flow
  
  This flow:
  1. Generates embedding for user query
  2. Performs vector search in Qdrant
  3. Performs graph search in Neo4J
  4. Merges and reranks results using reciprocal rank fusion
  5. Returns formatted context for LLM

inputs:
  - id: user_query
    type: STRING
    required: true
    description: User query to search for
  
  - id: top_k_vector
    type: INT
    defaults: 10
    description: Number of vector results to retrieve
  
  - id: top_k_graph
    type: INT
    defaults: 5
    description: Number of graph results to retrieve
  
  - id: rerank_top_k
    type: INT
    defaults: 5
    description: Number of final results after reranking

tasks:
  - id: hybrid_retrieval
    type: io.kestra.plugin.scripts.python.Script
    description: Execute LangGraph retrieval workflow
    docker:
      image: python:3.11-slim
      networkMode: kestra-network
    beforeCommands:
      - pip install --no-cache-dir qdrant-client neo4j langchain langchain-community langchain-ollama
    env:
      # Query parameters
      USER_QUERY: "{{ inputs.user_query }}"
      TOP_K_VECTOR: "{{ inputs.top_k_vector }}"
      TOP_K_GRAPH: "{{ inputs.top_k_graph }}"
      RERANK_TOP_K: "{{ inputs.rerank_top_k }}"
      
      # Qdrant Configuration
      QDRANT_URL: "{{ envs.QDRANT_URL }}"
      QDRANT_COLLECTION_NAME: "{{ envs.QDRANT_COLLECTION_NAME }}"
      
      # Neo4J Configuration
      NEO4J_URI: "{{ envs.NEO4J_URI }}"
      NEO4J_USER: "{{ envs.NEO4J_USER }}"
      NEO4J_PASSWORD: "{{ envs.NEO4J_PASSWORD }}"
      
      # Ollama Configuration (Local GPU Inference)
      OLLAMA_URL: "{{ envs.OLLAMA_URL }}"
      EMBEDDING_MODEL: "{{ envs.EMBEDDING_MODEL }}"
      EMBEDDING_DIMENSIONS: "{{ envs.EMBEDDING_DIMENSIONS }}"
    
    script: |
      import sys
      import os
      import json
      import logging
      
      # Configure logging
      logging.basicConfig(
          level=logging.INFO,
          format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
      )
      logger = logging.getLogger(__name__)
      
      # Add langgraph to path
      sys.path.insert(0, '/app/langgraph')
      
      try:
          from graphs.retrieval_graph import build_retrieval_graph
          from graphs.shared import RetrievalState
          
          logger.info("Building retrieval workflow...")
          graph = build_retrieval_graph()
          
          # Parse inputs
          user_query = os.getenv("USER_QUERY")
          top_k_vector = int(os.getenv("TOP_K_VECTOR", "10"))
          top_k_graph = int(os.getenv("TOP_K_GRAPH", "5"))
          rerank_top_k = int(os.getenv("RERANK_TOP_K", "5"))
          
          # Initialize state
          initial_state = RetrievalState(
              user_query=user_query,
              top_k_vector=top_k_vector,
              top_k_graph=top_k_graph,
              rerank_top_k=rerank_top_k,
              query_embedding=None,
              vector_results=[],
              graph_results=[],
              combined_results=[],
              reranked_results=[],
              context="",
              metadata={},
              error=None
          )
          
          logger.info("Starting hybrid retrieval...")
          logger.info(f"Query: {user_query[:100]}...")
          
          # Execute workflow
          final_state = graph.invoke(initial_state)
          
          # Check for errors
          if final_state.get("error"):
              logger.error(f"Retrieval failed: {final_state['error']}")
              sys.exit(1)
          
          # Extract results
          context = final_state.get("context", "")
          metadata = final_state.get("metadata", {})
          
          logger.info("=" * 60)
          logger.info("RETRIEVAL COMPLETED")
          logger.info("=" * 60)
          logger.info(f"Vector results: {len(final_state.get('vector_results', []))}")
          logger.info(f"Graph results: {len(final_state.get('graph_results', []))}")
          logger.info(f"Final sources: {metadata.get('num_sources', 0)}")
          logger.info(f"Context length: {len(context)} characters")
          logger.info("=" * 60)
          
          # Write context to file for Kestra to capture
          with open('/tmp/context.txt', 'w') as f:
              f.write(context)
          
          # Write metadata
          with open('/tmp/metadata.json', 'w') as f:
              json.dump(metadata, f)
          
          print(f"::{{set-output name=context::{context}}}")
          print(f"::{{set-output name=num_sources::{metadata.get('num_sources', 0)}}}")
          
      except Exception as e:
          logger.error(f"Workflow execution failed: {e}", exc_info=True)
          sys.exit(1)

outputs:
  - id: context
    type: STRING
    value: "{{ outputs.hybrid_retrieval.vars.context }}"
  
  - id: num_sources
    type: INT
    value: "{{ outputs.hybrid_retrieval.vars.num_sources }}"
  
  - id: query
    type: STRING
    value: "{{ inputs.user_query }}"
  
  - id: status
    type: STRING
    value: "completed"

